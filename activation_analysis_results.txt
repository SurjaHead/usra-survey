Activation Function Performance Analysis Report
Report generated on: 2025-05-29 22:59:46
======================================================================

Individual Model Performance Details
----------------------------------------------------------------------

Model: GNN_Cora
--------------------------------------------------
Top 5 performers by speed (avg activation time, ms):
Activation      | Avg Time (ms)  
---------------------------------
Mish            | 0.086          
ReLU            | 0.096          
LeakyReLU       | 0.098          
Tanh            | 0.102          
ELU             | 0.121          

Top 5 performers by accuracy (best_val_acc):
Activation      | Best Val Acc   
---------------------------------
SiLU            | 0.7540         
ELU             | 0.7320         
ReLU            | 0.7240         
GELU            | 0.7120         
Mish            | 0.7060         

==================================================

Model: GNN_PubMed
--------------------------------------------------
Top 5 performers by speed (avg activation time, ms):
Activation      | Avg Time (ms)  
---------------------------------
ReLU            | 0.054          
Tanh            | 0.062          
LeakyReLU       | 0.087          
ELU             | 0.097          
Mish            | 0.097          

Top 5 performers by accuracy (best_val_acc):
Activation      | Best Val Acc   
---------------------------------
Tanh            | 0.7900         
SiLU            | 0.7900         
Mish            | 0.7820         
ELU             | 0.7800         
GELU            | 0.7720         

==================================================

Model: ResNet18
--------------------------------------------------
Top 5 performers by speed (avg activation time, ms):
Activation      | Avg Time (ms)  
---------------------------------
SiLU            | 0.316          
Mish            | 0.344          
Sigmoid         | 0.860          
GELU            | 0.871          
ReLU            | 0.878          

Top 5 performers by accuracy (best_val_acc):
Activation      | Best Val Acc   
---------------------------------
SiLU            | 0.9205         
ReLU            | 0.9203         
Mish            | 0.9191         
GELU            | 0.9183         
LeakyReLU       | 0.9182         

==================================================

Model: MLP_MNIST
--------------------------------------------------
Top 8 performers by speed (avg activation time, ms):
Activation      | Avg Time (ms)  
---------------------------------
Sigmoid         | 0.123          
Mish            | 0.134          
Tanh            | 0.135          
ReLU            | 0.140          
LeakyReLU       | 0.142          
GELU            | 0.142          
SiLU            | 0.147          
ELU             | 0.148          

Top 8 performers by accuracy (best_val_acc):
Activation      | Best Val Acc   
---------------------------------
ELU             | 0.9829         
LeakyReLU       | 0.9809         
SiLU            | 0.9809         
Sigmoid         | 0.9807         
ReLU            | 0.9804         
Mish            | 0.9799         
GELU            | 0.9795         
Tanh            | 0.9488         

==================================================

Model: BERT
--------------------------------------------------
Top 5 performers by speed (avg activation time, ms):
Activation      | Avg Time (ms)  
---------------------------------
GELU            | 0.946          
ReLU            | 0.950          
LeakyReLU       | 0.952          
ELU             | 0.952          
Sigmoid         | 0.993          

Top 5 performers by accuracy (best_val_acc):
Activation      | Best Val Acc   
---------------------------------
GELU            | 0.9300         
ReLU            | 0.9232         
LeakyReLU       | 0.8463         
Tanh            | 0.5092         
Mish            | 0.5092         

==================================================


Overall Activation Function Performance Summary
======================================================================
This table shows how many times each activation function ranked in the top 5 
for speed (lowest average activation time) and accuracy (highest validation accuracy)
across all analyzed model types.

Activation      | Top 5 Speed Tally    | Top 5 Accuracy Tally  
---------------------------------------------------------------
ELU             | 3                    | 3                     
GELU            | 2                    | 4                     
LeakyReLU       | 4                    | 3                     
Mish            | 4                    | 4                     
ReLU            | 5                    | 4                     
SiLU            | 1                    | 4                     
Sigmoid         | 3                    | 1                     
Tanh            | 3                    | 2                     


Combined Top 5 Performance Ranking
======================================================================
This table sums the 'Top 5 Speed Tally' and 'Top 5 Accuracy Tally' 
to give an overall ranking based on total top 5 mentions.

Activation      | Total Top 5 Mentions     
-------------------------------------------
ReLU            | 9                        
Mish            | 8                        
LeakyReLU       | 7                        
ELU             | 6                        
GELU            | 6                        
SiLU            | 5                        
Tanh            | 5                        
Sigmoid         | 4                        

Report End
